docks:
  https://docs.confluent.io/debezium-connect-mysql-source/current/overview.html
# ---------------mysql installaion-------------------
docker run -it --rm --name mysql \
-p 4000:3306 \
-e MYSQL_ROOT_PASSWORD=debezium \
-e MYSQL_USER=mysqluser \
-e MYSQL_PASSWORD=mysqlpw debezium/example-mysql:0.9

username=mysqluser
pass=mysqlpw

# ---------------------------------
open database container

docker exec -it c-id bash
> mysql -u mysqluser -p
>mysqlpw (pass)


# Create the file register-mysql.json to store the following connector configuration:
# without avro
{
    "name": "inventory-connector2",
    "config": {
        "connector.class": "io.debezium.connector.mysql.MySqlConnector",
        "tasks.max": "1",
        "database.hostname": "10.36.37.178",
        "database.port": "3000",
        "database.user": "debezium",
        "database.password": "dbz",
        "database.server.id": "184054",
        "database.server.name": "dbserver1",
        "database.whitelist": "inventory",
        "database.history.kafka.bootstrap.servers": "10.36.37.178:9092",
        "database.history.kafka.topic": "schema-changes.inventory",
        "value.converter": "org.apache.kafka.connect.json.JsonConverter",
        "value.converter.schemas.enable": "false",
        "key.converter": "org.apache.kafka.connect.json.JsonConverter",
        "key.converter.schemas.enable": "false",
        "transforms": "unwrap",
        "transforms.unwrap.type": "io.debezium.transforms.ExtractNewRecordState",
        "transforms.unwrap.drop.tombstones": "false",
        "transforms.unwrap.delete.handling.mode": "rewrite",
        "transforms.unwrap.add.fields": "op,source.ts_ms,source.db,source.table.string"
            }
    }


---------for avro config----------------------
{
    "name": "inventory-connector2",
    "config": {
        "connector.class": "io.debezium.connector.mysql.MySqlConnector",
        "tasks.max": "1",
        "database.hostname": "10.36.37.178",
        "database.port": "3000",
        "database.user": "debezium",
        "database.password": "dbz",
        "database.server.id": "184054",
        "database.server.name": "dbserver1",
        "database.whitelist": "inventory",
        "database.history.kafka.bootstrap.servers": "10.36.37.178:9092",
        "database.history.kafka.topic": "schema-changes.inventory",
        "value.converter.schemas.enable": "false",
        "value.converter.schema.registry.url":"http://10.36.37.178:8081",
        "key.converter.schema.registry.url":"http://10.36.37.178:8081",
        "key.converter.schemas.enable": "false",
        "transforms": "unwrap",
        "transforms.unwrap.type": "io.debezium.transforms.ExtractNewRecordState",
        "transforms.unwrap.drop.tombstones": "false",
        "transforms.unwrap.delete.handling.mode": "rewrite",
        "transforms.unwrap.add.fields": "op,source.ts_ms,source.db,source.table.string"
            }
    }


-------------------------------

# upload this file into kafka connector
# you can create new table in this database
CREATE TABLE meta (
    PersonID int,
    LastName varchar(255),
    FirstName varchar(255),
    Address varchar(255),
    City varchar(255)
);

insert into meta(PersonID,LastName,FirstName,Address,City) values(1,"fwef","wewe","ewfjwekf","wefkjwe");



## rest api for kafka consumer(run where your kafka is running)
kafka-avro-console-consumer --bootstrap-server 10.36.37.178:9092 --topic dbserver1.inventory.meta --from-beginning 





